{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-31T00:59:37.757902600Z",
     "start_time": "2024-12-31T00:59:07.605617600Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cleaned_no_non_latin_words_split_2_Ambra.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Read DataFrame that needs to be tokenized\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcleaned_no_non_latin_words_split_2_Ambra.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpreprocess\u001B[39m(text):\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m# Make all text lowercase\u001B[39;00m\n\u001B[0;32m      9\u001B[0m     text \u001B[38;5;241m=\u001B[39m text\u001B[38;5;241m.\u001B[39mlower()\n",
      "File \u001B[1;32m~\\OneDrive - TU Eindhoven\\LAI-Assignment-Group22\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m    899\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    900\u001B[0m     dialect,\n\u001B[0;32m    901\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    908\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    909\u001B[0m )\n\u001B[0;32m    910\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 912\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - TU Eindhoven\\LAI-Assignment-Group22\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    574\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    576\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 577\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    580\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\OneDrive - TU Eindhoven\\LAI-Assignment-Group22\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1404\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1406\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1407\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - TU Eindhoven\\LAI-Assignment-Group22\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1660\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1661\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1662\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1663\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1664\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1665\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1666\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1667\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1668\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1670\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1671\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1672\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\OneDrive - TU Eindhoven\\LAI-Assignment-Group22\\lib\\site-packages\\pandas\\io\\common.py:859\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    854\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    855\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    856\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    857\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    858\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 859\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    860\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    861\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    863\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    865\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    866\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    867\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    868\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'cleaned_no_non_latin_words_split_2_Ambra.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read DataFrame that needs to be tokenized\n",
    "df = pd.read_csv('cleaned_no_non_latin_words_split_2_Ambra.csv')\n",
    "\n",
    "def preprocess(text):\n",
    "    # Make all text lowercase\n",
    "    text = text.lower()\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"  # Enclosed Characters\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    text = emoji_pattern.sub(\"\", text)\n",
    "    # Remove list indices like (a), (b), etc.\n",
    "    text = re.sub(r'\\(\\w\\)', '', text)\n",
    "    # Remove single letters except for \"I\", \"A\", \"U\" (and lowercase variants)\n",
    "    text = re.sub(r'\\b(?!(I|A|U|i|a|u)\\b)\\w\\b', '', text)\n",
    "    # Remove lone letters surrounded by punctuation, except i, o, u, a (case-insensitive)\n",
    "    text = re.sub(r'\\b(?!(i|o|u|a|I|O|U|A)\\b)[a-zA-Z]\\b(?=\\W|\\s|$)', '', text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize(text):\n",
    "    # Match acronyms, contractions, words, and punctuation as separate tokens\n",
    "    return re.findall(r'\\b(?:[A-Za-z]\\.)+[A-Za-z]\\b|(?:\\w+\\'\\w+)|\\w+|[^\\w\\s]', text)\n",
    "\n",
    "# Apply preprocessing and tokenization\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(preprocess)  # Apply preprocess to 'cleaned_text' column\n",
    "df['tokens'] = df['cleaned_text'].apply(tokenize)  # Apply tokenize to 'cleaned_text' column\n",
    "df.drop(columns=['post'], inplace=True)\n",
    "\n",
    "# Save the resulting DataFrame to a new CSV\n",
    "df.to_csv('tokens_non_latin_words_split_2_Ambra.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM (no n-grams) - Precision: 0.603572182673168 Recall: 0.5322772639868144 F1 Score: 0.5447215630353948\n",
      "Linear SVM with n-grams - Precision: 0.6196241573259396 Recall: 0.5357567988279461 F1 Score: 0.5455106361764036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 1 -CLEANED TEXT\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('cleaned_train_split_2_Ambra.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Apply preprocessing to training data\n",
    "train_data['cleaned_text'] = train_data['cleaned_text'].apply(preprocess)\n",
    "\n",
    "X_train_texts = train_data['cleaned_text']\n",
    "y_train_labels = train_data['nationality']\n",
    "X_test_texts = test_data['post']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_labels)\n",
    "y_test = label_encoder.transform(test_data['nationality'])\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2), tokenizer=tokenize)\n",
    "\n",
    "# Linear SVM (without n-grams)\n",
    "X_train = vectorizer.fit_transform(X_train_texts)\n",
    "X_test = vectorizer.transform(X_test_texts)\n",
    "\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(\"Linear SVM (no n-grams) - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)\n",
    "\n",
    "# Linear SVM with n-grams\n",
    "X_train_ngram = vectorizer_ngram.fit_transform(X_train_texts)\n",
    "X_test_ngram = vectorizer_ngram.transform(X_test_texts)\n",
    "\n",
    "svm.fit(X_train_ngram, y_train)\n",
    "y_pred_ngram = svm.predict(X_test_ngram)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_ngram, average='weighted')\n",
    "print(\"Linear SVM with n-grams - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-31T02:47:58.833027500Z",
     "start_time": "2024-12-31T00:59:37.773363700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM (no n-grams) - Precision: 0.5769436556804576 Recall: 0.5014192839483563 F1 Score: 0.5214500573956581\n",
      "Linear SVM with n-grams - Precision: 0.5973065273258047 Recall: 0.5202820254555444 F1 Score: 0.527169321828692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 2 -NO LATIN WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('cleaned_no_non_latin_words_split_2_Ambra.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Apply preprocessing to training data\n",
    "train_data['cleaned_text'] = train_data['cleaned_text'].apply(preprocess)\n",
    "\n",
    "X_train_texts = train_data['cleaned_text']\n",
    "y_train_labels = train_data['nationality']\n",
    "X_test_texts = test_data['post']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_labels)\n",
    "y_test = label_encoder.transform(test_data['nationality'])\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2), tokenizer=tokenize)\n",
    "\n",
    "# Linear SVM (without n-grams)\n",
    "X_train = vectorizer.fit_transform(X_train_texts)\n",
    "X_test = vectorizer.transform(X_test_texts)\n",
    "\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(\"Linear SVM (no n-grams) - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)\n",
    "\n",
    "# Linear SVM with n-grams\n",
    "X_train_ngram = vectorizer_ngram.fit_transform(X_train_texts)\n",
    "X_test_ngram = vectorizer_ngram.transform(X_test_texts)\n",
    "\n",
    "svm.fit(X_train_ngram, y_train)\n",
    "y_pred_ngram = svm.predict(X_test_ngram)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_ngram, average='weighted')\n",
    "print(\"Linear SVM with n-grams - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-31T04:37:19.473851400Z",
     "start_time": "2024-12-31T02:47:58.846162Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM (no n-grams) - Precision: 0.5878477261180246 Recall: 0.5061807526783262 F1 Score: 0.5200153817110377\n",
      "Linear SVM with n-grams - Precision: 0.6025949121748672 Recall: 0.505356652321216 F1 Score: 0.5175849503170623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 3 - GRAMMAR CORRECTED\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('cleaned_train_split_2_Ambra_with_grammar_correction.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Apply preprocessing to training data\n",
    "train_data['grammar_corrected_text'] = train_data['grammar_corrected_text'].apply(preprocess)\n",
    "\n",
    "X_train_texts = train_data['grammar_corrected_text']\n",
    "y_train_labels = train_data['nationality']\n",
    "X_test_texts = test_data['post']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_labels)\n",
    "y_test = label_encoder.transform(test_data['nationality'])\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2), tokenizer=tokenize)\n",
    "\n",
    "# Linear SVM (without n-grams)\n",
    "X_train = vectorizer.fit_transform(X_train_texts)\n",
    "X_test = vectorizer.transform(X_test_texts)\n",
    "\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(\"Linear SVM (no n-grams) - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)\n",
    "\n",
    "# Linear SVM with n-grams\n",
    "X_train_ngram = vectorizer_ngram.fit_transform(X_train_texts)\n",
    "X_test_ngram = vectorizer_ngram.transform(X_test_texts)\n",
    "\n",
    "svm.fit(X_train_ngram, y_train)\n",
    "y_pred_ngram = svm.predict(X_test_ngram)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_ngram, average='weighted')\n",
    "print(\"Linear SVM with n-grams - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-31T06:26:48.191593400Z",
     "start_time": "2024-12-31T04:37:19.483853Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM (no n-grams) - Precision: 0.5992783479896269 Recall: 0.5273326618441535 F1 Score: 0.538973204363394\n",
      "Linear SVM with n-grams - Precision: 0.6110525330616329 Recall: 0.5282483289076092 F1 Score: 0.5369931953459186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 4 - ONLY ENGLISH LANGUAGE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('cleaned_train_split_2_Ambra_exp4.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Apply preprocessing to training data\n",
    "train_data['cleaned_text'] = train_data['cleaned_text'].apply(preprocess)\n",
    "\n",
    "X_train_texts = train_data['cleaned_text']\n",
    "y_train_labels = train_data['nationality']\n",
    "X_test_texts = test_data['post']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_labels)\n",
    "y_test = label_encoder.transform(test_data['nationality'])\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2), tokenizer=tokenize)\n",
    "\n",
    "# Linear SVM (without n-grams)\n",
    "X_train = vectorizer.fit_transform(X_train_texts)\n",
    "X_test = vectorizer.transform(X_test_texts)\n",
    "\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(\"Linear SVM (no n-grams) - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)\n",
    "\n",
    "# Linear SVM with n-grams\n",
    "X_train_ngram = vectorizer_ngram.fit_transform(X_train_texts)\n",
    "X_test_ngram = vectorizer_ngram.transform(X_test_texts)\n",
    "\n",
    "svm.fit(X_train_ngram, y_train)\n",
    "y_pred_ngram = svm.predict(X_test_ngram)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_ngram, average='weighted')\n",
    "print(\"Linear SVM with n-grams - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-31T08:05:12.942041100Z",
     "start_time": "2024-12-31T06:26:48.202083100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 13\u001B[0m\n\u001B[0;32m     10\u001B[0m chunk_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1000\u001B[39m\n\u001B[0;32m     11\u001B[0m train_data_chunks \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokenized_correct_grammar_depolluted_test_data.csv\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     12\u001B[0m                                 encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlatin1\u001B[39m\u001B[38;5;124m\"\u001B[39m, engine\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpython\u001B[39m\u001B[38;5;124m\"\u001B[39m, chunksize\u001B[38;5;241m=\u001B[39mchunk_size, on_bad_lines\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarn\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 13\u001B[0m train_data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data_chunks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m test_data_chunks \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mData/test_data.csv\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     16\u001B[0m                                encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlatin1\u001B[39m\u001B[38;5;124m\"\u001B[39m, engine\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpython\u001B[39m\u001B[38;5;124m\"\u001B[39m, chunksize\u001B[38;5;241m=\u001B[39mchunk_size, on_bad_lines\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarn\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     17\u001B[0m test_data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat(test_data_chunks, ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\OneDrive - TU Eindhoven\\LAI-Assignment-Group22\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:372\u001B[0m, in \u001B[0;36mconcat\u001B[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[0m\n\u001B[0;32m    369\u001B[0m elif copy and using_copy_on_write():\n\u001B[0;32m    370\u001B[0m     copy = False\n\u001B[1;32m--> 372\u001B[0m op = _Concatenator(\n\u001B[0;32m    373\u001B[0m     objs,\n\u001B[0;32m    374\u001B[0m     axis=axis,\n\u001B[0;32m    375\u001B[0m     ignore_index=ignore_index,\n\u001B[0;32m    376\u001B[0m     join=join,\n\u001B[0;32m    377\u001B[0m     keys=keys,\n\u001B[0;32m    378\u001B[0m     levels=levels,\n\u001B[0;32m    379\u001B[0m     names=names,\n\u001B[0;32m    380\u001B[0m     verify_integrity=verify_integrity,\n\u001B[0;32m    381\u001B[0m     copy=copy,\n\u001B[0;32m    382\u001B[0m     sort=sort,\n\u001B[0;32m    383\u001B[0m )\n\u001B[0;32m    385\u001B[0m return op.get_result()\n",
      "File \u001B[1;32m~\\OneDrive - TU Eindhoven\\LAI-Assignment-Group22\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:426\u001B[0m, in \u001B[0;36m_Concatenator.__init__\u001B[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001B[0m\n\u001B[0;32m    424\u001B[0m     objs \u001B[38;5;241m=\u001B[39m [objs[k] \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m keys]\n\u001B[0;32m    425\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 426\u001B[0m     objs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mobjs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    428\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(objs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    429\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo objects to concatenate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\OneDrive - TU Eindhoven\\LAI-Assignment-Group22\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1624\u001B[0m, in \u001B[0;36mTextFileReader.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1622\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[0;32m   1623\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1624\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_chunk\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1625\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m   1626\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\OneDrive - TU Eindhoven\\LAI-Assignment-Group22\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1733\u001B[0m, in \u001B[0;36mTextFileReader.get_chunk\u001B[1;34m(self, size)\u001B[0m\n\u001B[0;32m   1731\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n\u001B[0;32m   1732\u001B[0m     size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnrows \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_currow)\n\u001B[1;32m-> 1733\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - TU Eindhoven\\LAI-Assignment-Group22\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1697\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[0;32m   1698\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1699\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[0;32m   1700\u001B[0m     (\n\u001B[0;32m   1701\u001B[0m         index,\n\u001B[0;32m   1702\u001B[0m         columns,\n\u001B[0;32m   1703\u001B[0m         col_dict,\n\u001B[1;32m-> 1704\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[0;32m   1705\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[0;32m   1706\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1707\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1708\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\OneDrive - TU Eindhoven\\LAI-Assignment-Group22\\lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:251\u001B[0m, in \u001B[0;36mPythonParser.read\u001B[1;34m(self, rows)\u001B[0m\n\u001B[0;32m    245\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread\u001B[39m(\n\u001B[0;32m    246\u001B[0m     \u001B[38;5;28mself\u001B[39m, rows: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    247\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[\n\u001B[0;32m    248\u001B[0m     Index \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, Sequence[Hashable] \u001B[38;5;241m|\u001B[39m MultiIndex, Mapping[Hashable, ArrayLike]\n\u001B[0;32m    249\u001B[0m ]:\n\u001B[0;32m    250\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 251\u001B[0m         content \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_lines\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    252\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    253\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_first_chunk:\n",
      "File \u001B[1;32m~\\OneDrive - TU Eindhoven\\LAI-Assignment-Group22\\lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:1121\u001B[0m, in \u001B[0;36mPythonParser._get_lines\u001B[1;34m(self, rows)\u001B[0m\n\u001B[0;32m   1117\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(rows \u001B[38;5;241m+\u001B[39m rows_to_skip):\n\u001B[0;32m   1118\u001B[0m     \u001B[38;5;66;03m# assert for mypy, data is Iterator[str] or None, would\u001B[39;00m\n\u001B[0;32m   1119\u001B[0m     \u001B[38;5;66;03m# error in next\u001B[39;00m\n\u001B[0;32m   1120\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1121\u001B[0m     new_rows\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1123\u001B[0m len_new_rows \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(new_rows)\n\u001B[0;32m   1124\u001B[0m new_rows \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_remove_skipped_rows(new_rows)\n",
      "\u001B[1;31mMemoryError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Final experiment with all data\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('cleaned_train_split_2_Ambra_exp4.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "X_train_texts = train_data['grammar_corrected_text']\n",
    "y_train_labels = train_data['nationality']\n",
    "X_test_texts = test_data['post']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_labels)\n",
    "y_test = label_encoder.transform(test_data['nationality'])\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2), tokenizer=tokenize)\n",
    "\n",
    "# Linear SVM (without n-grams)\n",
    "X_train = vectorizer.fit_transform(X_train_texts)\n",
    "X_test = vectorizer.transform(X_test_texts)\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train_smote, y_train_smote)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(\"Linear SVM (no n-grams) with SMOTE - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)\n",
    "\n",
    "# Linear SVM with n-grams\n",
    "X_train_ngram = vectorizer_ngram.fit_transform(X_train_texts)\n",
    "X_test_ngram = vectorizer_ngram.transform(X_test_texts)\n",
    "\n",
    "# Apply SMOTE to n-gram data\n",
    "X_train_ngram_smote, y_train_ngram_smote = smote.fit_resample(X_train_ngram, y_train)\n",
    "\n",
    "svm.fit(X_train_ngram_smote, y_train_ngram_smote)\n",
    "y_pred_ngram = svm.predict(X_test_ngram)\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_ngram, average='weighted')\n",
    "print(\"Linear SVM with n-grams and SMOTE - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
