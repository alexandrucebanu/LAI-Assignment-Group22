{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-31T00:59:37.757902600Z",
     "start_time": "2024-12-31T00:59:07.605617600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read DataFrame that needs to be tokenized\n",
    "df = pd.read_csv('cleaned_no_non_latin_words_split_2_Ambra.csv')\n",
    "\n",
    "def preprocess(text):\n",
    "    # Make all text lowercase\n",
    "    text = text.lower()\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"  # Enclosed Characters\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    text = emoji_pattern.sub(\"\", text)\n",
    "    # Remove list indices like (a), (b), etc.\n",
    "    text = re.sub(r'\\(\\w\\)', '', text)\n",
    "    # Remove single letters except for \"I\", \"A\", \"U\" (and lowercase variants)\n",
    "    text = re.sub(r'\\b(?!(I|A|U|i|a|u)\\b)\\w\\b', '', text)\n",
    "    # Remove lone letters surrounded by punctuation, except i, o, u, a (case-insensitive)\n",
    "    text = re.sub(r'\\b(?!(i|o|u|a|I|O|U|A)\\b)[a-zA-Z]\\b(?=\\W|\\s|$)', '', text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize(text):\n",
    "    # Match acronyms, contractions, words, and punctuation as separate tokens\n",
    "    return re.findall(r'\\b(?:[A-Za-z]\\.)+[A-Za-z]\\b|(?:\\w+\\'\\w+)|\\w+|[^\\w\\s]', text)\n",
    "\n",
    "# Apply preprocessing and tokenization\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(preprocess)  # Apply preprocess to 'cleaned_text' column\n",
    "df['tokens'] = df['cleaned_text'].apply(tokenize)  # Apply tokenize to 'cleaned_text' column\n",
    "df.drop(columns=['post'], inplace=True)\n",
    "\n",
    "# Save the resulting DataFrame to a new CSV\n",
    "df.to_csv('tokens_non_latin_words_split_2_Ambra.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM (no n-grams) - Precision: 0.603572182673168 Recall: 0.5322772639868144 F1 Score: 0.5447215630353948\n",
      "Linear SVM with n-grams - Precision: 0.6196241573259396 Recall: 0.5357567988279461 F1 Score: 0.5455106361764036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 1 -CLEANED TEXT\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('cleaned_train_split_2_Ambra.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Apply preprocessing to training data\n",
    "train_data['cleaned_text'] = train_data['cleaned_text'].apply(preprocess)\n",
    "\n",
    "X_train_texts = train_data['cleaned_text']\n",
    "y_train_labels = train_data['nationality']\n",
    "X_test_texts = test_data['post']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_labels)\n",
    "y_test = label_encoder.transform(test_data['nationality'])\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2), tokenizer=tokenize)\n",
    "\n",
    "# Linear SVM (without n-grams)\n",
    "X_train = vectorizer.fit_transform(X_train_texts)\n",
    "X_test = vectorizer.transform(X_test_texts)\n",
    "\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(\"Linear SVM (no n-grams) - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)\n",
    "\n",
    "# Linear SVM with n-grams\n",
    "X_train_ngram = vectorizer_ngram.fit_transform(X_train_texts)\n",
    "X_test_ngram = vectorizer_ngram.transform(X_test_texts)\n",
    "\n",
    "svm.fit(X_train_ngram, y_train)\n",
    "y_pred_ngram = svm.predict(X_test_ngram)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_ngram, average='weighted')\n",
    "print(\"Linear SVM with n-grams - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-31T02:47:58.833027500Z",
     "start_time": "2024-12-31T00:59:37.773363700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM (no n-grams) - Precision: 0.5769436556804576 Recall: 0.5014192839483563 F1 Score: 0.5214500573956581\n",
      "Linear SVM with n-grams - Precision: 0.5973065273258047 Recall: 0.5202820254555444 F1 Score: 0.527169321828692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 2 -NO LATIN WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('cleaned_no_non_latin_words_split_2_Ambra.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Apply preprocessing to training data\n",
    "train_data['cleaned_text'] = train_data['cleaned_text'].apply(preprocess)\n",
    "\n",
    "X_train_texts = train_data['cleaned_text']\n",
    "y_train_labels = train_data['nationality']\n",
    "X_test_texts = test_data['post']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_labels)\n",
    "y_test = label_encoder.transform(test_data['nationality'])\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2), tokenizer=tokenize)\n",
    "\n",
    "# Linear SVM (without n-grams)\n",
    "X_train = vectorizer.fit_transform(X_train_texts)\n",
    "X_test = vectorizer.transform(X_test_texts)\n",
    "\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(\"Linear SVM (no n-grams) - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)\n",
    "\n",
    "# Linear SVM with n-grams\n",
    "X_train_ngram = vectorizer_ngram.fit_transform(X_train_texts)\n",
    "X_test_ngram = vectorizer_ngram.transform(X_test_texts)\n",
    "\n",
    "svm.fit(X_train_ngram, y_train)\n",
    "y_pred_ngram = svm.predict(X_test_ngram)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_ngram, average='weighted')\n",
    "print(\"Linear SVM with n-grams - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-31T04:37:19.473851400Z",
     "start_time": "2024-12-31T02:47:58.846162Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM (no n-grams) - Precision: 0.5878477261180246 Recall: 0.5061807526783262 F1 Score: 0.5200153817110377\n",
      "Linear SVM with n-grams - Precision: 0.6025949121748672 Recall: 0.505356652321216 F1 Score: 0.5175849503170623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 3 - GRAMMAR CORRECTED\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('cleaned_train_split_2_Ambra_with_grammar_correction.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Apply preprocessing to training data\n",
    "train_data['grammar_corrected_text'] = train_data['grammar_corrected_text'].apply(preprocess)\n",
    "\n",
    "X_train_texts = train_data['grammar_corrected_text']\n",
    "y_train_labels = train_data['nationality']\n",
    "X_test_texts = test_data['post']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_labels)\n",
    "y_test = label_encoder.transform(test_data['nationality'])\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2), tokenizer=tokenize)\n",
    "\n",
    "# Linear SVM (without n-grams)\n",
    "X_train = vectorizer.fit_transform(X_train_texts)\n",
    "X_test = vectorizer.transform(X_test_texts)\n",
    "\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(\"Linear SVM (no n-grams) - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)\n",
    "\n",
    "# Linear SVM with n-grams\n",
    "X_train_ngram = vectorizer_ngram.fit_transform(X_train_texts)\n",
    "X_test_ngram = vectorizer_ngram.transform(X_test_texts)\n",
    "\n",
    "svm.fit(X_train_ngram, y_train)\n",
    "y_pred_ngram = svm.predict(X_test_ngram)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_ngram, average='weighted')\n",
    "print(\"Linear SVM with n-grams - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-31T06:26:48.191593400Z",
     "start_time": "2024-12-31T04:37:19.483853Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM (no n-grams) - Precision: 0.5992783479896269 Recall: 0.5273326618441535 F1 Score: 0.538973204363394\n",
      "Linear SVM with n-grams - Precision: 0.6110525330616329 Recall: 0.5282483289076092 F1 Score: 0.5369931953459186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvesc\\Desktop\\Language and AI\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 4 - ONLY ENGLISH LANGUAGE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('cleaned_train_split_2_Ambra_exp4.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Apply preprocessing to training data\n",
    "train_data['cleaned_text'] = train_data['cleaned_text'].apply(preprocess)\n",
    "\n",
    "X_train_texts = train_data['cleaned_text']\n",
    "y_train_labels = train_data['nationality']\n",
    "X_test_texts = test_data['post']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_labels)\n",
    "y_test = label_encoder.transform(test_data['nationality'])\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2), tokenizer=tokenize)\n",
    "\n",
    "# Linear SVM (without n-grams)\n",
    "X_train = vectorizer.fit_transform(X_train_texts)\n",
    "X_test = vectorizer.transform(X_test_texts)\n",
    "\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(\"Linear SVM (no n-grams) - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)\n",
    "\n",
    "# Linear SVM with n-grams\n",
    "X_train_ngram = vectorizer_ngram.fit_transform(X_train_texts)\n",
    "X_test_ngram = vectorizer_ngram.transform(X_test_texts)\n",
    "\n",
    "svm.fit(X_train_ngram, y_train)\n",
    "y_pred_ngram = svm.predict(X_test_ngram)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_ngram, average='weighted')\n",
    "print(\"Linear SVM with n-grams - Precision:\", precision, \"Recall:\", recall, \"F1 Score:\", f1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-31T08:05:12.942041100Z",
     "start_time": "2024-12-31T06:26:48.202083100Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
